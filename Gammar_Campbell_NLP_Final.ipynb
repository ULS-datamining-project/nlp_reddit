{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "458c663b",
   "metadata": {},
   "source": [
    "# Gammar/Campbell NLP assignment\n",
    "\n",
    "## Aims\n",
    "\n",
    "To analyse reddit posts to discover trends in healthy eating on food subreddits. We will attempt the following:\n",
    "\n",
    "- Download a series of posts from reddit using their API\n",
    "- Characterise these posts using exploratory analysis\n",
    "- Predict whether a subreddit is a healthy eating subreddit based on its post content\n",
    "\n",
    "\n",
    "## Why Reddit?\n",
    "\n",
    "Reddit is a social media site where users post either links around the web or text content that they've written themselves. These posts are subject to a an upvote/downvote system which causes popular submissions to rise higher onto users' feeds. Older posts have lower weighted upvotes, resulting in a constant feed of new, highly regarded content.\n",
    "\n",
    "Reddit's style is casual, but serious. Posts are generally typed out in full sentences with emojis and reaction images being comparatively rare. This makes it an ideal candidate for natural language processing.\n",
    "\n",
    "## Read in data\n",
    "\n",
    "This section is rendered as markdown rather than a code block as it takes approximately 40 minutes to run. We've provided CSVs to save you the trouble. These are to be placed in a `data/` subdirectory. Multireddits are groups of subreddits grouped by a common theme. We did a Google search for a \"food multireddit\" and [came across a general one](https://www.reddit.com/r/Cooking/comments/cg7lha/misc_heres_all_of_the_food_related_subreddits_i/) made by Reddit user [Nomeii](https://www.reddit.com/user/Nomeii/).\n",
    "\n",
    "We used the [Python Reddit API Wrapper (praw)](https://praw.readthedocs.io/en/stable/index.html) to loop through each of these subreddits picking the top 1000 most upvoted posts from the past year. We saved this as a csv and will be using this as the base of our of our future analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e322b8d6",
   "metadata": {},
   "source": [
    "```python\n",
    "import praw\n",
    "import pandas as pd\n",
    "import datetime as time\n",
    "from colorama import Fore, Style\n",
    "from pathlib import Path\n",
    "\n",
    "# Make data directory for newly written data if it doesn't already exist\n",
    "DATA_DIRECTORY = \"data/\"\n",
    "Path(DATA_DIRECTORY).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "POST_TIME_PERIOD = \"year\"\n",
    "\n",
    "# Number of posts per subreddit to pull\n",
    "N_TITLES = 1000\n",
    "\n",
    "#Obtained from praw.ini file in working directory\n",
    "reddit = praw.Reddit(\"uls-healthyeating\", check_for_async=False)\n",
    "\n",
    "# Chose this multireddit as it has many food subreddits\n",
    "food_multireddit = reddit.multireddit(name=\"food\", redditor = \"nomeii\")\n",
    "\n",
    "top_dict = {\"subreddit\" : [],\n",
    "            \"title\" : [],\n",
    "            \"is_self\" : [],\n",
    "            \"selftext\" : [],\n",
    "            \"author\" : [],\n",
    "            \"url\" : [],\n",
    "            \"score\" : [],\n",
    "            \"upvote_ratio\" : [],\n",
    "            \"n_gilded\" : [],\n",
    "            \"num_comments\" : [],\n",
    "            \"permalink\" : [],\n",
    "            \"created_utc\" : []\n",
    "            }\n",
    "\n",
    "# Cycle over each of the subreddits, grab posts and append it to the\n",
    "# global dictionary\n",
    "\n",
    "for index,subreddit in enumerate(food_multireddit.subreddits):\n",
    "    subreddit_name = subreddit.display_name\n",
    "    \n",
    "    #Subreddits to appear in red\n",
    "    print(\"\\n[\", time.datetime.now(), \"]\", f\"{Fore.RED}****{subreddit_name}****{Style.RESET_ALL}\")\n",
    "    print(f\"Subreddit number: {index}\")\n",
    "    \n",
    "    subreddit_data = subreddit.top(limit=N_TITLES, time_filter=POST_TIME_PERIOD)\n",
    "    \n",
    "    for post in subreddit_data:\n",
    "        \n",
    "        top_dict[\"subreddit\"].append(subreddit_name)\n",
    "        top_dict[\"title\"].append(post.title)\n",
    "        top_dict[\"is_self\"].append(post.is_self)\n",
    "        top_dict[\"selftext\"].append(post.selftext)\n",
    "        top_dict[\"author\"].append(None if post.author is None else post.author.name)\n",
    "        top_dict[\"url\"].append(post.url)\n",
    "        top_dict[\"score\"].append(post.score)\n",
    "        top_dict[\"upvote_ratio\"].append(post.upvote_ratio)\n",
    "        top_dict[\"n_gilded\"].append(post.gilded)\n",
    "        top_dict[\"num_comments\"].append(post.num_comments)\n",
    "        top_dict[\"permalink\"].append(post.permalink)\n",
    "        top_dict[\"created_utc\"].append(post.created_utc)\n",
    "    \n",
    "# Combine dictionary into one large dataframe    \n",
    "top_df = pd.DataFrame(top_dict)\n",
    "\n",
    "top_df.to_csv(DATA_DIRECTORY + \"reddit_data.csv\", index = False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0969d2a",
   "metadata": {},
   "source": [
    "## Data description\n",
    "\n",
    "The Reddit data is a 13-column dataset with ~70K rows. It contains among others: post titles, author names, subreddit names, upvote scores, post time and post text where available. We retrieved it from the Reddit API on 2023-03-08 as of the most recent update.\n",
    "\n",
    "We've also manually made a mapping between each of the subreddits and our expert opinion as to if the subreddit relates to healthy eating. Subreddits explicitly about healthy eating and plant-based diets were considered healthy as well as home-cooking subreddits. We made this mapping to facilitate supervised learning in our models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6623dafc",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7f9ce3",
   "metadata": {},
   "source": [
    "## Modelling healthy-eating subreddits\n",
    "\n",
    "For this task, we wanted to perform a supervised classification model. We decided on logistic regression from `scikit-learn` as we could validate the results with a more statistical approach as well. We want to see if we can predict if a post comes from a \"healthy\" subreddit based only on its upvote ratio, whether it's a self post or its sentiment.\n",
    "\n",
    "### Sentiment analysis\n",
    "\n",
    "We needed more variables and also needed a way to include our text variables. A positive/negative sentiment would constitue an additional variable and there are already pretrained models that allowing us to perform this without the need to process billions of records to create a satisfactory model. We used the `distilbert-base-uncased-finetuned-sst-2-english` model on the post titles in our data. Again, this process took over an hour on m machine so the following is markdown rather than a code chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f66181",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import datetime as time\n",
    "\n",
    "# We're using sentiment analysis to classify post titles so that we can use \n",
    "# that to help us to predict posts as healthy or not\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\",\n",
    "                      \"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Read in our reddit data and healthiness classifier\n",
    "raw_data = pd.read_csv(\"data/raw_reddit_data.csv\")\n",
    "subreddit_health = pd.read_csv(\"data/subreddit_health.csv\")\n",
    "\n",
    "# Merge assessment of subreddit health with subreddit data\n",
    "reddit_data = raw_data.merge(subreddit_health, on=\"subreddit\")\n",
    "\n",
    "# Run sentiment analysis on each element\n",
    "## This takes a long time so we make note of the start and stop time\n",
    "print(time.datetime.now())\n",
    "reddit_data.loc[:, ('label', 'score')] = reddit_data.title.apply(\n",
    "    classifier).explode().apply(pd.Series)\n",
    "print(time.datetime.now())\n",
    "\n",
    "# This takes a long time so we make _sure_ that the result is saved\n",
    "reddit_data.to_csv(\"data/reddit_data_sentiment.csv\", index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e84a239",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "We're using the `LogisticRegression` model from `sklearn` for the model `is_subreddit_healthy ~ upvote_ratio + is_self_post + has_positive_title`.\n",
    "Our data has \"Healthy\", \"Neutral\" and \"Unhealthy\" categories, but to keep the model simple, we've lumped Neutral and Unhealthy together into one. We split the data into a training set and a validation set with a 20% split to validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bd7e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from statsmodels.formula.api import logit\n",
    "\n",
    "sentiment_data = pd.read_csv(\"data/reddit_data_sentiment.csv\")\n",
    "\n",
    "def sentiment2int(sentiment):\n",
    "    if sentiment == \"NEGATIVE\":\n",
    "        return(False)\n",
    "    if sentiment == \"POSITIVE\":\n",
    "        return(True)\n",
    "\n",
    "\n",
    "def nature2int(nature):\n",
    "    if nature == \"Unhealthy\":\n",
    "        return(False)\n",
    "    if nature == \"Neutral\":\n",
    "        return(False)\n",
    "    if nature == \"Healthy\":\n",
    "        return(True)\n",
    "\n",
    "X = sentiment_data.loc[:, (\"upvote_ratio\", \"is_self\", \"label\")]\n",
    "#Change sentiments to logical\n",
    "X[\"title_is_positive\"] = [sentiment2int(sentiment) for sentiment in X[\"label\"]]\n",
    "X.drop(\"label\", axis = 1, inplace = True)\n",
    "\n",
    "Y = pd.Series([nature2int(nature) for nature in sentiment_data.loc[:, \"nature\"]], name = \"nature\")\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2023)\n",
    "\n",
    "\n",
    "base_lr = LogisticRegression(solver='liblinear') #liblinear dumps everything in one category\n",
    "\n",
    "model = base_lr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09425d26",
   "metadata": {},
   "source": [
    "It's here that we have our first surprise when we look at the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f645789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(Y_train, model.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5156232",
   "metadata": {},
   "source": [
    "The most effective model is one where we just flat out assume that all posts are characterised as unhealthy. I didn't believe it at first, but if we look at how to predicts on its training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88945785",
   "metadata": {},
   "outputs": [],
   "source": [
    "any(model.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a20b7c2",
   "metadata": {},
   "source": [
    "They are definitely all False. Maybe we'll get different results on the validation set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c236833",
   "metadata": {},
   "outputs": [],
   "source": [
    "any(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d6a887",
   "metadata": {},
   "source": [
    "Same result. Perhaps there is no relationship between the variables. We don't really have a measure of that in our model. In order work that out, we'll have to use more classical statistical techniques.\n",
    "\n",
    "### Statistical logistic regression\n",
    "\n",
    "The more classical approach use a logit curve. We're going to use the framework from `statsmodels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fabb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_data = pd.concat([Y_train, X_train], axis = 1)\n",
    "\n",
    "# Convert bools to int so statsmodels can process it\n",
    "logit_data[\"nature\"] = logit_data.nature.apply(int)\n",
    "logit_data[\"is_self\"] = logit_data.is_self.apply(int)\n",
    "logit_data[\"title_is_positive\"] = logit_data.title_is_positive.apply(int)\n",
    "\n",
    "logit_model = logit(\"nature ~ upvote_ratio + is_self + title_is_positive\", logit_data)\n",
    "#logit_model = sm.Logit(np.array(Y_train),X_train)\n",
    "\n",
    "result = logit_model.fit()\n",
    "result.pred_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303f61aa",
   "metadata": {},
   "source": [
    "Hmm, same result. That's a good sign. Let's see if we were correct about there being no relationship between the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951420d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4435cca",
   "metadata": {},
   "source": [
    "Not at all. There seems to be a strong relationship with each of them (except `is_self`). It seems like a relatively complex model that leads to a simple result. Not what we were hoping for, but an interesting result nonetheless. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1ff211",
   "metadata": {},
   "source": [
    "## Future avenues of research"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
