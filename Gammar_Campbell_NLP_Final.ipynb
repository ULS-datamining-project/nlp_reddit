{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3dca113",
   "metadata": {},
   "source": [
    "# Gammar/Campbell NLP assignment\n",
    "\n",
    "## Aims\n",
    "\n",
    "To analyse reddit posts\n",
    "\n",
    "## Why Reddit?\n",
    "\n",
    "\n",
    "\n",
    "## Read in data\n",
    "\n",
    "This section is rendered as markdown rather than a code block as it takes approximately 40 minutes to run. We've provided CSVs to save you the trouble. These are to be placed in a `data/` subdirectory. Multireddits are groups of subreddits grouped by a common theme. We did a Google search for a \"food multireddit\" and [came across a general one](https://www.reddit.com/r/Cooking/comments/cg7lha/misc_heres_all_of_the_food_related_subreddits_i/) made by Reddit user [Nomeii](https://www.reddit.com/user/Nomeii/).\n",
    "\n",
    "We used the [Python Reddit API Wrapper (praw)](https://praw.readthedocs.io/en/stable/index.html) to loop through each of these subreddits picking the top 1000 most upvoted posts from the past year. We saved this as a csv and will be using this as the base of our of our future analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5e70b4",
   "metadata": {},
   "source": [
    "```python\n",
    "import praw\n",
    "import pandas as pd\n",
    "import datetime as time\n",
    "from colorama import Fore, Style\n",
    "from pathlib import Path\n",
    "\n",
    "# Make data directory for newly written data if it doesn't already exist\n",
    "DATA_DIRECTORY = \"data/\"\n",
    "Path(DATA_DIRECTORY).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "POST_TIME_PERIOD = \"year\"\n",
    "\n",
    "# Number of posts per subreddit to pull\n",
    "N_TITLES = 1000\n",
    "\n",
    "#Obtained from praw.ini file in working directory\n",
    "reddit = praw.Reddit(\"uls-healthyeating\", check_for_async=False)\n",
    "\n",
    "# Chose this multireddit as it has many food subreddits\n",
    "food_multireddit = reddit.multireddit(name=\"food\", redditor = \"nomeii\")\n",
    "\n",
    "top_dict = {\"subreddit\" : [],\n",
    "            \"title\" : [],\n",
    "            \"is_self\" : [],\n",
    "            \"selftext\" : [],\n",
    "            \"author\" : [],\n",
    "            \"url\" : [],\n",
    "            \"score\" : [],\n",
    "            \"upvote_ratio\" : [],\n",
    "            \"n_gilded\" : [],\n",
    "            \"num_comments\" : [],\n",
    "            \"permalink\" : [],\n",
    "            \"created_utc\" : []\n",
    "            }\n",
    "\n",
    "# Cycle over each of the subreddits, grab posts and append it to the\n",
    "# global dictionary\n",
    "\n",
    "for index,subreddit in enumerate(food_multireddit.subreddits):\n",
    "    subreddit_name = subreddit.display_name\n",
    "    \n",
    "    #Subreddits to appear in red\n",
    "    print(\"\\n[\", time.datetime.now(), \"]\", f\"{Fore.RED}****{subreddit_name}****{Style.RESET_ALL}\")\n",
    "    print(f\"Subreddit number: {index}\")\n",
    "    \n",
    "    subreddit_data = subreddit.top(limit=N_TITLES, time_filter=POST_TIME_PERIOD)\n",
    "    \n",
    "    for post in subreddit_data:\n",
    "        \n",
    "        top_dict[\"subreddit\"].append(subreddit_name)\n",
    "        top_dict[\"title\"].append(post.title)\n",
    "        top_dict[\"is_self\"].append(post.is_self)\n",
    "        top_dict[\"selftext\"].append(post.selftext)\n",
    "        top_dict[\"author\"].append(None if post.author is None else post.author.name)\n",
    "        top_dict[\"url\"].append(post.url)\n",
    "        top_dict[\"score\"].append(post.score)\n",
    "        top_dict[\"upvote_ratio\"].append(post.upvote_ratio)\n",
    "        top_dict[\"n_gilded\"].append(post.gilded)\n",
    "        top_dict[\"num_comments\"].append(post.num_comments)\n",
    "        top_dict[\"permalink\"].append(post.permalink)\n",
    "        top_dict[\"created_utc\"].append(post.created_utc)\n",
    "    \n",
    "# Combine dictionary into one large dataframe    \n",
    "top_df = pd.DataFrame(top_dict)\n",
    "\n",
    "top_df.to_csv(DATA_DIRECTORY + \"reddit_data.csv\", index = False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19898438",
   "metadata": {},
   "source": [
    "## Data description\n",
    "\n",
    "The Reddit data is a 13-column dataset with ~70K rows. It contains among others: post titles, author names, subreddit names, upvote scores, post time and post text where available."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
